{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = cuisine_df2.copy()\n",
    "\n",
    "def get_ing(x):\n",
    "    return spoon_df2.loc[spoon_df2['Ingredient'].str.contains(x), 'Aisle'].iloc[0]\n",
    "\n",
    "df4['Aisle2'] = df4['Ingredient'].apply(get_ing)\n",
    "\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoon_df2.loc[spoon_df2['Ingredient'].str.contains(x), 'Aisle'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_id.pop('cajun_creole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spoon CSV\n",
    "spoon_df = pd.read_csv('../SpoonacularIngredientListComplete.csv')\n",
    "spoon_df = spoon_df.fillna(\"Other\")\n",
    "spoon_df.head()\n",
    "\n",
    "#load spoon CSV\n",
    "spoon_df = pd.read_csv('../SpoonacularIngredientListComplete.csv')\n",
    "spoon_df = spoon_df.fillna(\"Other\")\n",
    "spoon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_df['Len'] = cuisine_df['Ingredient'].str.len()\n",
    "cuisine_df['Radius'] = cuisine_df['Len'].map(radius_dic)\n",
    "cuisne_df = cuisine_df.dropna()\n",
    "\n",
    "cuisine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.merge(cuisine_df, spoon_df, on='Ingredient', how= 'left')\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {'vietnamese coriander': 'coriander', 'thai chile': \"chile\", 'sweet chili sauce': \"chili sauce\", 'beansprouts': \"fresh bean sprouts\", 'red bell pepper': \"red bell peppers\", 'virgin olive oil': \"extra virgin olive oil\", 'peaches': \"peach\", 'bourbon whiskey': \"bourbon\", 'chopped pecans': \"pecan pieces\", 'yellow corn meal': \"cornmeal\", 'milk': \"low fat milk\", 'jalapeno chilies': \"jalapeno\", 'beef rib short': \"short ribs\", 'korean chile paste': \"chili paste\", 'soybean sprouts': \"sprouts\", 'sushi rice': \"rice\", 'ricotta cheese': \"low fat ricotta cheese\", 'mozzarella cheese': \"part-skim mozzarella cheese\", 'pinenuts': \"pine nuts\",  'irish whiskey': \"whiskey\",  'cut oats': \"steel cut oats\",  'yukon gold potatoes': \"yukon gold potato\",  'russet': \"russet potatoes\",  'ground coriander': \"coriander\",  'pitas': \"pita\",  'corn starch': \"cornstarch\",  'chopped celery': \"celery\",  'cooked rice': \"rice\",  'dried currants': \"currants\",  'rolled oats': \"oats\",  6: \"F\",  6: \"F\",  6: \"F\",  6: \"F\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['standard_ingredient'] = output_df['Ingredient'].map(di).fillna(output_df['Ingredient'])\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame({ 'col1': np.random.choice( range(1,9), 10 ) })\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = x_df['col1'].map(di).fillna(x_df['col1'])\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoon_df2 = pd.DataFrame({'Ingredient':['romaine lettuce', 'black olives', 'grape tomatoes', 'bell pepper', 'black pepper'], 'Aisle': ['bake','veg', 'veg', 'fruit', 'fruit']})\n",
    "cuisine_df2 = pd.DataFrame({'Ingredient':['romaine lettuce', 'ground black pepper', 'tomatoes']})\n",
    "spoon_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.merge(cuisine_df2, spoon_df2, on='Ingredient', how= 'inner')\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "\n",
    "fullstring = \"ground black pepper\"\n",
    "substring = \"black pepper\"\n",
    "\n",
    "if search(substring, fullstring):\n",
    "    print (\"Found!\")\n",
    "else:\n",
    "    print (\"Not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = spoon_df2['Ingredient']\n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w), \" : \", lemmatizer.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import chi2\n",
    "# import numpy as np\n",
    "# N = 3\n",
    "# for cuisine, category_id in sorted(category_to_id.items()):\n",
    "#     features_chi2 = chi2(features, labels == category_id)\n",
    "#     indices = np.argsort(features_chi2[0])\n",
    "#     feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\n",
    "#     unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "# #     bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#     print(\"# '{}':\".format(cuisine))\n",
    "#     print(\"  . Most correlated:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "# #     print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "dler = nltk.downloader.Downloader()\n",
    "dler._update_index()\n",
    "dler.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "N = 20\n",
    "for cuisine, category_id in sorted(category_to_id.items()):\n",
    "    #run chi squared test \n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    list = unigrams[-N:]\n",
    "    #save resulting ingredients to dataframe\n",
    "    cuisine_df = pd.DataFrame(unigrams[-N:], columns=['Ingredient'])\n",
    "    cuisine_df['Ingredient']=cuisine_df.Ingredient.replace('_', ' ', regex=True)\n",
    "    #save dataframe to csv\n",
    "    data_folder = os.path.join(\"cuisine_top_ingredients_data\")\n",
    "    filename = os.path.join(data_folder, cuisine + '_ingredients.csv')\n",
    "    cuisine_df.to_csv(filename, index=False)\n",
    "    print (cuisine + ':' + str(list))\n",
    "    print('---------------------------------')\n",
    "print(\"csv's saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cuisine, category_id in sorted(category_to_id.items()):\n",
    "#     features_chi2 = chi2(features, labels == category_id)\n",
    "#     indices = np.argsort(features_chi2[0])\n",
    "#     feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\n",
    "#     unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#     bigrams = [v for v in feature_names if len(v.split(' ')) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "list_df = train_clean_df[train_df.ingredients = 'list']\n",
    "list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_summary1 = train_clean_df.ingredients.value_counts()\n",
    "ing_summary1 = ing_summary1.to_frame()\n",
    "# ing_summary = ing_summary1.reset_index()\n",
    "# ing_summary = ing_summary.rename(columns = {'index':'cuisine', 'cuisine': 'count'})\n",
    "ing_summary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = pd.DataFrame({'values': [' [romaine lettuce, black olives, grape tomatoes]', '[plain flour, ground pepper, salt, tomatoes]']})\n",
    "test_list['values'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df.ingredients\n",
    "train_clean_df.ingredients.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# pattern = re.compile(r'\\s+')\n",
    "# train_clean_df['text2'] = re.sub(pattern, '', train_clean_df['text'])\n",
    "\n",
    "# # train_clean_df['text2'] = train_clean_df['text'].replace(\" \", \"\")\n",
    "# train_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mouse Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]\n",
    "\n",
    "docs1 = [[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "], [\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]]\n",
    "\n",
    "docs2 = pd.DataFrame({ 'text': docs})\n",
    "\n",
    "docs3 = pd.DataFrame({ 'text': docs1})\n",
    "docs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs3['B'] = docs3['text'].apply(lambda x:','.join([str(i) for i in x]))\n",
    "docs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category_to_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd45fce7de45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcategory_to_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'category_to_id' is not defined"
     ]
    }
   ],
   "source": [
    "category_to_id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X = features\n",
    "y = labels\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_df = pd.DataFrame(X_new)\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_id in sorted(category_to_id.items()):\n",
    "    print(category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(train_clean_df.text).toarray()\n",
    "labels = df.category_id\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_clean_df['text'], train_clean_df['cuisine'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf_NB_TFIDF = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 10))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "clf = ComplementNB()\n",
    "clf.fit(X, y)\n",
    "print(clf.predict(X[2:3]))\n",
    "print(X)\n",
    "print(X[2:3])\n",
    "print(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\n",
    "processed_features = vectorizer.fit_transform(train_clean_df.ingredients).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in class nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Outcome\"]\n",
    "target_names = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Outcome\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler and y_scaler models\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that k: 13 seems to be the best choice for this dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('k=13 Test Acc: %.3f' % knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build our text\n",
    "text = \"\"\"Yesterday I went fishing. I don't fish that often, \n",
    "so I didn't catch any fish. I was told I'd enjoy myself, \n",
    "but it didn't really seem that fun.\"\"\"\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "matrix = vectorizer.fit_transform(docs2.text)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counts = pd.DataFrame(matrix.toarray(),\n",
    "                      columns=vectorizer.get_feature_names())\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(train_clean_df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counts = pd.DataFrame(matrix.toarray(),\n",
    "                      columns=vectorizer.get_feature_names())\n",
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pythonadv)",
   "language": "python",
   "name": "pythonadv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
